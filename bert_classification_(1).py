# -*- coding: utf-8 -*-
"""BERT_Classification (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nN1LFnVZVUwAFMxNJbk8-TFCKGh243gz

# Fine-Tuning BERT for Text Classification

# Dataset Preparation
## Load the CSV
"""

# Import required libraries
import pandas as pd
import numpy as np
import re
import torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW # Import AdamW from torch.optim
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/BertDataset/comments.csv'
df = pd.read_csv(file_path)

"""## check"""

# prompt: print top 5 rows

print(df.head())

print(f"Total rows in CSV: {len(df)}")
print(df.head())

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)

"""## Preprocess Text"""

# Lowercase all labels
df['label'] = df['label'].str.lower()

# Clean text - remove only specified symbols
def clean_text(text):
    return re.sub(r'[?&#$%@*^]', '', str(text))

df['cleaned_text'] = df['text'].apply(clean_text)

# Convert labels to numerical values
label_map = {'positive': 0, 'mixed': 1, 'negative': 2}
df['label_encoded'] = df['label'].map(label_map)

# Shuffle the DataFrame
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# Check if cleaning produced valid text
print("Sample texts after cleaning:")
print(df['cleaned_text'].head())
print(f"Empty texts after cleaning: {df['cleaned_text'].isnull().sum()}")

print("Unique labels in dataset:", df['label'].unique())

print("NaN in label_encoded:", df['label_encoded'].isna().sum())

# Remove rows with NaN values in 'label_encoded' column
df = df.dropna(subset=['label_encoded'])

# Verify if there are any NaN values left
print("NaN values in 'label_encoded' after dropping rows:", df['label_encoded'].isna().sum())

test_text = "هل هذا مثال؟ #اختبار"
cleaned = re.sub(r'[?&#$%@*^]', '', test_text)
print(f"Before: {test_text}\nAfter: {cleaned}")

"""##Split Data (70% Train, 30% Test"""

train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['cleaned_text'],
    df['label_encoded'],
    test_size=0.3,
    random_state=42,
    stratify=df['label_encoded']  # Maintain class balance
)

"""# BERT Model Setup

## Choose Model
"""

# Load Arabic BERT model and tokenizer
model_name = 'asafaya/bert-base-arabic'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3,
    output_attentions=False,
    output_hidden_states=False
)

"""## Tokenization

##Determine max_len
"""

all_lengths = [len(tokenizer.encode(text)) for text in df['cleaned_text']]
max_len = max(all_lengths) + 2  # Add 2 for [CLS] and [SEP]
print(f"Using max_len: {max_len}")

train_encodings = tokenizer(
    train_texts.tolist(),
    truncation=True,
    padding='max_length',
    max_length=max_len,
    return_tensors='pt'
)

test_encodings = tokenizer(
    test_texts.tolist(),
    truncation=True,
    padding='max_length',
    max_length=max_len,
    return_tensors='pt'
)

"""# Training Configuration
## Batch Size Selection
"""

# Create PyTorch datasets
train_dataset = TensorDataset(
    train_encodings['input_ids'],
    train_encodings['attention_mask'],
    torch.tensor(train_labels.tolist(), dtype=torch.long) # Ensure labels are LongTensor
)

test_dataset = TensorDataset(
    test_encodings['input_ids'],
    test_encodings['attention_mask'],
    torch.tensor(test_labels.tolist(), dtype=torch.long) # Ensure labels are LongTensor
)

# Determine batch size (start with 8 and increase)
batch_size = 8
gradient_accumulation_steps = 8

"""## Hyperparameters"""

# Create data loaders
train_dataloader = DataLoader(
    train_dataset,
    sampler=RandomSampler(train_dataset),
    batch_size=batch_size
)

test_dataloader = DataLoader(
    test_dataset,
    sampler=SequentialSampler(test_dataset),
    batch_size=batch_size
)

"""# Train Setup"""

# prompt: print labels to check format

print(df['label'].unique())
print(df['label_encoded'].unique())

# prompt: check if lables 1d shape
# Check if labels are 1D
print(f"Shape of train_labels: {train_labels.shape}")
print(f"Shape of test_labels: {test_labels.shape}")

if train_labels.ndim == 1 and test_labels.ndim ==1:
    print("Labels are 1D arrays")
else:
    print("Labels are NOT 1D arrays")

# prompt: convert float lables into integeres, print new lables

# Convert float labels to integers
df['label_encoded'] = df['label_encoded'].astype(int)

# Print the new labels
print(df['label_encoded'])

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
total_steps = len(train_dataloader) // gradient_accumulation_steps
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps
)

#Training loop (1 epoch)
model.train()
for epoch in range(1):
    total_loss = 0
    for step, batch in enumerate(train_dataloader):
        batch = tuple(t.to(device) for t in batch)
        inputs = {
            'input_ids': batch[0],
            'attention_mask': batch[1],
            'labels': batch[2]
        }

        # Convert labels to one-hot encoding
        labels = batch[2]
        one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=3).float().to(device)
        inputs['labels'] = one_hot_labels

        outputs = model(**inputs)
        loss = outputs.loss
        loss = loss / gradient_accumulation_steps
        loss.backward()
        total_loss += loss.item()

        if (step + 1) % gradient_accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

    avg_train_loss = total_loss / len(train_dataloader)
    print(f"Epoch {epoch + 1}, Average Loss: {avg_train_loss:.4f}")

"""#  Evaluation

# Test on Custom Sentences
"""

custom_sentences = [
    "الفيلم كان رائعاً!",                   # Positive
    "الخدمة مقبولة لكن بطيئة",              # Mixed
    "أسوأ تجربة في حياتي"                   # Negative
]

cleaned_sentences = [clean_text(s) for s in custom_sentences]
encoded_inputs = tokenizer(
    cleaned_sentences,
    truncation=True,
    padding='max_length',
    max_length=max_len,
    return_tensors='pt'
).to(device)

with torch.no_grad():
    outputs = model(**encoded_inputs)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=1)

reverse_label_map = {v: k for k, v in label_map.items()}
for i, sentence in enumerate(custom_sentences):
    print(f"Sentence: {sentence}")
    print(f"Predicted sentiment: {reverse_label_map[predictions[i].item()]}")
    print("-" * 50)

"""##"""